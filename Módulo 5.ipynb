{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Módulo 5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vk-kbZgFZRLv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlcibMKA_a-j"
      },
      "source": [
        "# Módulo 5 - Aprendizado não-supervisionado e por reforço\n",
        "\n",
        "Neste notebook, implementaremos algoritmos de aprendizado não-supervisionado e por reforço. Na primeira parte, veremos a implementação do algoritmo k-means. Na segunda parte, veremos o algoritmo Q-learning. Em ambos os casos, o objetivo é entender como estes algoritmos funcionam. Ao final, você deverá realizar os exercícios propostos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqWhkCoq_jyp"
      },
      "source": [
        "## Parte 1 - Aprendizado não-supervisionado\n",
        "\n",
        "Vamos começar utilizando o algoritmo k-means para agrupar as amostras de plantas iris em três clusters sem utilizar os rótulos destas amostras. Utilizaremos a implementação do k-means disponível na biblioteca [scikit-learn](https://scikit-learn.org/stable/).\n",
        "\n",
        "Primeiramente, precisamos importar algumas bibliotecas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFECGWDaTlEr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "np.random.seed(98237)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYNhDXpY-Iq"
      },
      "source": [
        "Em seguida, podemos importar o dataset e fazer alguns pré-processamentos básicos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HEEVn-HTmlV"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgoZtVABZEBe"
      },
      "source": [
        "Nosso próximo passo é executar o algoritmo k-means. Para isso, podemos criar uma instância do algoritmo especificando o número de clusters desejados. Em seguida, basta chamar o método `fit`, que recebe os dados (`X`) e executa o algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwugD35qTopc"
      },
      "source": [
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "kmeans.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-vymu0_Zdjw"
      },
      "source": [
        "Como a tarefa é simples, a execução é rápida. \n",
        "\n",
        "Agora podemos criar um gráfico para permitir a visualização dos clusters resultantes. Note que este dataset possui quatro features. Considerando que podemos criar gráficos de no máximo três dimensões, precisamos selecionar quais features serão plotadas. No nosso caso, vamos ignorar o comprimento da sépala e utilizar as demais features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q67VcDX_VGVi"
      },
      "source": [
        "print('Available features:', iris.feature_names)\n",
        "\n",
        "# for plotting, select only three features (as we can only plot three dimensions)\n",
        "features_to_plot = [3,0,2]\n",
        "feature_names = np.choose(features_to_plot, iris.feature_names)\n",
        "\n",
        "print('Features selected for plotting:',feature_names)\n",
        "\n",
        "assert len(features_to_plot) == 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LcTQCEPaE9K"
      },
      "source": [
        "Antes de criar o gráfico, vamos definir as cores que utilizaremos para os clusters. Basta criar a lista abaixo com as cores (cuidando para que especificar uma cor para cada cluster)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqwvrDVcaGGH"
      },
      "source": [
        "colours = ['r', 'g', 'b'] \n",
        "\n",
        "assert len(colours) == k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM_7h9efaXLL"
      },
      "source": [
        "Agora sim podemos criar o gráfico utilizando os dados (`X`), os clusters encontrados (`kmeans.labels_`), e as cores definidas (`colours`). \n",
        "Note que, além de plotar os dados, estamos plotando também os centróides dos clusters (`kmeans.cluster_centers_`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu6y27z2XW9l"
      },
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "\n",
        "y_colours = np.choose(kmeans.labels_, colours) # colour of the data points\n",
        "\n",
        "ax.scatter(X[:, features_to_plot[0]], X[:, features_to_plot[1]], X[:, features_to_plot[2]], c=y_colours, edgecolor='k', s=50)\n",
        "ax.scatter(kmeans.cluster_centers_[:, 3], kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], edgecolor='k', s=500, marker=(5,1), c=colours, depthshade=False)\n",
        "\n",
        "ax.set_xlabel(feature_names[0])\n",
        "ax.set_ylabel(feature_names[1])\n",
        "ax.set_zlabel(feature_names[2])\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkHiaEaWawst"
      },
      "source": [
        "Finalmente, para poder verificar o quão bom foi nosso algoritmo, podemos plotar também as classes reais de cada amostra. O processo aqui é parecido com o gráfico anterior. A diferença é que vamos definir a cor de cada amostra com base nas classes reais (`y`) ao invés das encontradas pelo algoritmo (`kmeans.labels_`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyex9lhSS2y8"
      },
      "source": [
        "# Plot the ground truth\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "\n",
        "y_colours = np.choose(y, colours) # colour of the data points\n",
        "\n",
        "ax.scatter(X[:, features_to_plot[0]], X[:, features_to_plot[1]], X[:, features_to_plot[2]], c=y_colours, edgecolor='k', s=50)\n",
        "\n",
        "ax.set_xlabel(feature_names[0])\n",
        "ax.set_ylabel(feature_names[1])\n",
        "ax.set_zlabel(feature_names[2])\n",
        "\n",
        "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
        "    ax.text3D(X[y == label, 3].mean()-0.3, X[y == label, 0].mean()+1.3, X[y == label, 2].mean(), name, horizontalalignment='center', bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No1KPnDjbk8z"
      },
      "source": [
        "Como pode ser observado, os resultados obtidos são muito próximos dos rótulos reais. De fato, conforme visto abaixo, apenas 10.67% das amostras foram agrupadas de forma incorreta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRECjdRGblaW"
      },
      "source": [
        "y_out = kmeans.labels_ # predicted labels\n",
        "#y_adj = np.choose(y_out, [0,1,2]) # uncomment if you need to align predicted and true label numbers\n",
        "wrong = sum([ int(y[i] != y_out[i]) for i in range(len(y))]) # count mismatches\n",
        "\n",
        "print('True labels: %s' % y)\n",
        "print('k-means labels: %s' % y_out)\n",
        "print('Wrong classifications: %d (%.2f%%)' % (wrong, wrong/1.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nZOqoUOzaf0"
      },
      "source": [
        "Finalmente, de modo a entender onde o algoritmo errou, podemos analisar a matriz de confusão gerada. O código abaixo faz precisamente isto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j99pWuwZzPEF"
      },
      "source": [
        "confusion_matrix(y, y_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05KgcqRpyZcD"
      },
      "source": [
        "### Exercício 1\n",
        "\n",
        "Utilize o algoritmo k-means visto na parte 1 para agrupar os dígitos do dataset MNIST (use o método [load_digit](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)) em 10 clusters. Ao final, mostre algumas imagens de cada cluster para permitir uma melhor análise dos resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjOO1JuyamY"
      },
      "source": [
        "# sua resposta aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FFEviT_xilF"
      },
      "source": [
        "## Parte 2 - Aprendizado por reforço\n",
        "\n",
        "Na segunda parte deste notebook, implementaremos o algoritmo de aprendizado por reforço chamado Q-learning para resolver problemas fornecidos pelo [Open AI Gym](http://gym.openai.com/envs/).\n",
        "\n",
        "Primeiramente, precisamos importar algumas bibliotecas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wclDBSIhx526"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install pygame\n",
        "import pygame\n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk-J4vQgx6QY"
      },
      "source": [
        "Em nosso exemplo, tentaremos resolver o problema chamado Cliff Walking, apresentado na figura abaixo. Neste problema, o agente parte de S e deve chegar em G de uma forma eficiente, mas evitando o penhasco. Para cada passo, o agente recebe uma recompensa de -1. Caso o agente caia no penhasco, a recompensa é -100. O episódio termina quando o agente chega em G ou cai no penhasco. Naturalmente, o objetivo é chegar em G o mais rapidamente possível para maximizar sua recompensa.\n",
        "\n",
        "![alt text](https://i.imgur.com/2IobMYw.png)\n",
        "\n",
        "Para utilizar este ambiente com o Gym, basta executar o comando abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBE5wf-Gx-FH"
      },
      "source": [
        "cliff_env = gym.make('CliffWalking-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj-gLEHcx-Pe"
      },
      "source": [
        "O ambiente pode ser reiniciado com a função `reset`. Podemos imprimir o estado atual do ambiente com a função `render`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utpWGky4x-X6"
      },
      "source": [
        "cliff_env.reset()\n",
        "cliff_env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5fPINTFx-fb"
      },
      "source": [
        "O código abaixo implementa o algoritmo Q-learning. O pseudocódigo do Q-learning é apresentado abaixo.\n",
        "\n",
        "![alt text](https://i.imgur.com/9yydGca.png)\n",
        "\n",
        "Em nossa implementação, o algoritmo recebe um ambiente, uma quantidade de episódios, além dos parâmetros usuais $\\alpha$ (taxa de aprendizado), $\\gamma$ (taxa de desconto) e $\\epsilon$ (taxa de exploração)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvl4qGIx-m8"
      },
      "source": [
        "def Qlearning(environment, num_episodes=100, alpha=0.3, gamma=0.9, epsilon=1.0, decay_epsilon=0.1, max_epsilon=1.0, min_epsilon=0.01):\n",
        "  \n",
        "  # initializing the Q-table\n",
        "  Q = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
        "\n",
        "  # additional lists to keep track of reward and epsilon values\n",
        "  rewards = []\n",
        "  epsilons = []\n",
        "\n",
        "  # episodes\n",
        "  for episode in range(num_episodes):\n",
        "      \n",
        "      # reset the environment to start a new episode\n",
        "      state = environment.reset()\n",
        "\n",
        "      # reward accumulated along episode\n",
        "      accumulated_reward = 0\n",
        "      \n",
        "      # steps within current episode\n",
        "      for step in range(100):\n",
        "          \n",
        "          # epsilon-greedy action selection\n",
        "          # exploit with probability 1-epsilon\n",
        "          if np.random.uniform(0, 1) > epsilon:\n",
        "              action = np.argmax(Q[state,:])\n",
        "              \n",
        "          # explore with probability epsilon\n",
        "          else:\n",
        "              action = environment.action_space.sample()\n",
        "              \n",
        "          # perform the action and observe the new state and corresponding reward\n",
        "          new_state, reward, done, info = environment.step(action)\n",
        "          \n",
        "\n",
        "          # update the Q-table\n",
        "          Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
        "          \n",
        "          # update the accumulated reward\n",
        "          accumulated_reward += reward\n",
        "\n",
        "          # update the current state\n",
        "          state = new_state\n",
        "          \n",
        "          # end the episode when it is done\n",
        "          if done == True:\n",
        "              break\n",
        "      \n",
        "      # decay exploration rate to ensure that the agent exploits more as it becomes experienced\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_epsilon*episode)\n",
        "      \n",
        "      # update the lists of rewards and epsilons\n",
        "      rewards.append(accumulated_reward)\n",
        "      epsilons.append(epsilon)\n",
        "\n",
        "  # render the environment\n",
        "  environment.render()\n",
        "    \n",
        "  # return the list of accumulated reward along episodes\n",
        "  return rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J7nCxb-x-ub"
      },
      "source": [
        "Para utilizar o Q-learning, basta chamá-lo como segue. No exemplo abaixo, vamos executar o algoritmo por 100 episódios. Ao final, calculamos a recompensa média total (ao longo de todos episódios) e a final (ao longo dos últimos 10 episódios)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74oCsAkVx-1j"
      },
      "source": [
        "num_episodes=100\n",
        "alpha=0.3\n",
        "gamma=0.9\n",
        "epsilon=1.0\n",
        "decay_epsilon=0.1\n",
        "\n",
        "# run Q-learning\n",
        "rewards = Qlearning(cliff_env, num_episodes, alpha, gamma, epsilon, decay_epsilon)\n",
        "\n",
        "# print results\n",
        "print (\"Average reward (all episodes): \" + str(sum(rewards)/num_episodes))\n",
        "print (\"Average reward (last 10 episodes): \" + str(sum(rewards[-10:])/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIn6uG-Nx-8Q"
      },
      "source": [
        "Para avaliar o desempenho do algoritmo, podemos criar um gráfico da recompensa total por episódio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PBPnIpwx_Dc"
      },
      "source": [
        "plt.plot(range(num_episodes), rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Accumulated reward along episodes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bdBKRP8x_Mi"
      },
      "source": [
        "Com base no gráfico acima, podemos observar que o algoritmo converge para uma recompensa próxima de zero. Considerando que a recompensa máxima neste ambiente é -13, podemos dizer que o algoritmo convergiu para uma política próxima da ótima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEr33xqwp0Ki"
      },
      "source": [
        "### Exercício 2\n",
        "\n",
        "Ajuste os parâmetros do algoritmo Q-learning para obter melhores resultados. Em seguida, rode o algoritmo com a melhor política aprendida, ou seja, escolhendo as ações de forma gulosa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB2l_eRnp0Wh"
      },
      "source": [
        "# sua resposta aqui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk-kbZgFZRLv"
      },
      "source": [
        "## Referências\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4LN3BZCzJyI"
      },
      "source": [
        "* https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html\n",
        "* https://machinelearningmastery.com/clustering-algorithms-with-python/"
      ]
    }
  ]
}